{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "description of dataset [here](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'age',\n",
    "    'sex',\n",
    "    'body_mass_index',\n",
    "    'average_blood_pressure',\n",
    "    's1',\n",
    "    's2',\n",
    "    's3',\n",
    "    's4',\n",
    "    's5',\n",
    "    's6'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['data']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>average_blood_pressure</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.639623e-16</td>\n",
       "      <td>1.309912e-16</td>\n",
       "      <td>-8.013951e-16</td>\n",
       "      <td>1.289818e-16</td>\n",
       "      <td>-9.042540e-17</td>\n",
       "      <td>1.301121e-16</td>\n",
       "      <td>-4.563971e-16</td>\n",
       "      <td>3.863174e-16</td>\n",
       "      <td>-3.848103e-16</td>\n",
       "      <td>-3.398488e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.072256e-01</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-9.027530e-02</td>\n",
       "      <td>-1.123996e-01</td>\n",
       "      <td>-1.267807e-01</td>\n",
       "      <td>-1.156131e-01</td>\n",
       "      <td>-1.023071e-01</td>\n",
       "      <td>-7.639450e-02</td>\n",
       "      <td>-1.260974e-01</td>\n",
       "      <td>-1.377672e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.729927e-02</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-3.422907e-02</td>\n",
       "      <td>-3.665645e-02</td>\n",
       "      <td>-3.424784e-02</td>\n",
       "      <td>-3.035840e-02</td>\n",
       "      <td>-3.511716e-02</td>\n",
       "      <td>-3.949338e-02</td>\n",
       "      <td>-3.324879e-02</td>\n",
       "      <td>-3.317903e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.383060e-03</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-7.283766e-03</td>\n",
       "      <td>-5.670611e-03</td>\n",
       "      <td>-4.320866e-03</td>\n",
       "      <td>-3.819065e-03</td>\n",
       "      <td>-6.584468e-03</td>\n",
       "      <td>-2.592262e-03</td>\n",
       "      <td>-1.947634e-03</td>\n",
       "      <td>-1.077698e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.807591e-02</td>\n",
       "      <td>5.068012e-02</td>\n",
       "      <td>3.124802e-02</td>\n",
       "      <td>3.564384e-02</td>\n",
       "      <td>2.835801e-02</td>\n",
       "      <td>2.984439e-02</td>\n",
       "      <td>2.931150e-02</td>\n",
       "      <td>3.430886e-02</td>\n",
       "      <td>3.243323e-02</td>\n",
       "      <td>2.791705e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.107267e-01</td>\n",
       "      <td>5.068012e-02</td>\n",
       "      <td>1.705552e-01</td>\n",
       "      <td>1.320442e-01</td>\n",
       "      <td>1.539137e-01</td>\n",
       "      <td>1.987880e-01</td>\n",
       "      <td>1.811791e-01</td>\n",
       "      <td>1.852344e-01</td>\n",
       "      <td>1.335990e-01</td>\n",
       "      <td>1.356118e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age           sex  body_mass_index  average_blood_pressure  \\\n",
       "count  4.420000e+02  4.420000e+02     4.420000e+02            4.420000e+02   \n",
       "mean  -3.639623e-16  1.309912e-16    -8.013951e-16            1.289818e-16   \n",
       "std    4.761905e-02  4.761905e-02     4.761905e-02            4.761905e-02   \n",
       "min   -1.072256e-01 -4.464164e-02    -9.027530e-02           -1.123996e-01   \n",
       "25%   -3.729927e-02 -4.464164e-02    -3.422907e-02           -3.665645e-02   \n",
       "50%    5.383060e-03 -4.464164e-02    -7.283766e-03           -5.670611e-03   \n",
       "75%    3.807591e-02  5.068012e-02     3.124802e-02            3.564384e-02   \n",
       "max    1.107267e-01  5.068012e-02     1.705552e-01            1.320442e-01   \n",
       "\n",
       "                 s1            s2            s3            s4            s5  \\\n",
       "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02   \n",
       "mean  -9.042540e-17  1.301121e-16 -4.563971e-16  3.863174e-16 -3.848103e-16   \n",
       "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
       "min   -1.267807e-01 -1.156131e-01 -1.023071e-01 -7.639450e-02 -1.260974e-01   \n",
       "25%   -3.424784e-02 -3.035840e-02 -3.511716e-02 -3.949338e-02 -3.324879e-02   \n",
       "50%   -4.320866e-03 -3.819065e-03 -6.584468e-03 -2.592262e-03 -1.947634e-03   \n",
       "75%    2.835801e-02  2.984439e-02  2.931150e-02  3.430886e-02  3.243323e-02   \n",
       "max    1.539137e-01  1.987880e-01  1.811791e-01  1.852344e-01  1.335990e-01   \n",
       "\n",
       "                 s6  \n",
       "count  4.420000e+02  \n",
       "mean  -3.398488e-16  \n",
       "std    4.761905e-02  \n",
       "min   -1.377672e-01  \n",
       "25%   -3.317903e-02  \n",
       "50%   -1.077698e-03  \n",
       "75%    2.791705e-02  \n",
       "max    1.356118e-01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = pd.DataFrame(X, columns=columns)\n",
    "df_x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Normally\n",
    "\n",
    "Cross Validating. \n",
    "We will use `KFold` class to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_hat):\n",
    "    # TODO implement rmse\n",
    "    return \n",
    "\n",
    "def print_all_metrics(y_true, y_hat, type='train'):\n",
    "    print('''\n",
    "    ....{} metrics....\n",
    "    rmse: {:.2f}\n",
    "    mae: {:.2f},\n",
    "    r2: {:.2f}\n",
    "    '''.format(\n",
    "        type,\n",
    "        rmse(y_true, y_hat),\n",
    "        mean_absolute_error(y_true, y_hat),\n",
    "        r2_score(y_true, y_hat)\n",
    "    ))\n",
    "    \n",
    "def all_metrics(y_true, y_hat):\n",
    "    return (\n",
    "        rmse(y_true, y_hat),\n",
    "        mean_absolute_error(y_true, y_hat),\n",
    "        r2_score(y_true, y_hat) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 142\n",
    "folds = KFold(5, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cv(folds, mdl):\n",
    "    cv_metrics = list()\n",
    "    # TODO impl\n",
    "    return pd.DataFrame(cv_metrics, columns=['cv_idx', 'rmse', 'mae', 'r2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.02436312772539\n",
      "    mae: 44.74745118158389,\n",
      "    r2: 0.5145594310441097\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 47.45269989280756\n",
      "    mae: 38.0110385192209,\n",
      "    r2: 0.5129960721305509\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.221493080425205\n",
      "    mae: 43.20290250040651,\n",
      "    r2: 0.5133532947316353\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 54.83009629950939\n",
      "    mae: 43.96187104635424,\n",
      "    r2: 0.5157971190066343\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.766894627525296\n",
      "    mae: 42.57888151257503,\n",
      "    r2: 0.5340127681563123\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 56.73593621666708\n",
      "    mae: 46.679995084432285,\n",
      "    r2: 0.43008067559481755\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.77401832383665\n",
      "    mae: 42.42384288310336,\n",
      "    r2: 0.51855474235312\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 56.91569994210364\n",
      "    mae: 47.20352947312387,\n",
      "    r2: 0.49517302931379437\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.98861976924631\n",
      "    mae: 42.82267709480363,\n",
      "    r2: 0.5167496968732601\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 56.14411213975221\n",
      "    mae: 45.84871679191414,\n",
      "    r2: 0.5079492646114012\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "cv_metrics = do_cv(folds, LinearRegression())\n",
    "\n",
    "cv_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging:  Bootstrap Aggregrating\n",
    "\n",
    "## Main Idea\n",
    "\n",
    "Bagging could be done by simply **train several weak predictor and take the average of each predictors**.\n",
    "\n",
    "Bagging comes from name: *Bootstrap Aggregating*. Bootstraping in statistics is an estimation of sampling distribution by taking random samples with replacement$^{5}$. Bagging predictor uses *bootstrap* because it literally train on different algorithm with same data${^6}$, thus it is sampling with replacement / bootstraping.\n",
    "\n",
    "\n",
    "> A critical factor in whether bagging will improve accuracy is the stability of the procedure for constructing $\\varphi$. If changes in $\\mathcal{L}$  i.e. a replicate $\\mathcal{L}$, produces small changes in $\\varphi$, then $\\varphi_B$ will be close to $\\varphi$. Improvement will occur for unstable procedures where a small change in $\\mathcal{L}$ can result in large changes in $\\varphi$ $^6$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathcal{L}$ is the learning sets / training set / training data\n",
    "- $\\varphi$ is the predictor\n",
    "- $\\varphi_B$ is the predictor trained on bootstrap sample of data\n",
    "\n",
    "\n",
    "\n",
    "What does it means? It means that Bagging will excel if there is a small change in training set $\\mathcal{L}$, results in big big change in prediction of $\\varphi$.\n",
    "\n",
    "\n",
    "## What Algorithms Could be Combined by Bagging?\n",
    "\n",
    "> neural nets, classification and regression trees, and subset selection\n",
    "\n",
    "\n",
    "## Toy Regressors\n",
    "\n",
    "For this purpose we will use 3 toy regressors: `LinearRegression`, `Lasso`, and `Ridge` \n",
    "\n",
    "$$\n",
    "f(x) =  \\sum_{m=0}^{M} \\frac{1}{M} * h_m(x)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $m$ is the model index\n",
    "\n",
    "- $h(x)$ is the prediction for model $h_m$ given the input $x$\n",
    "\n",
    "- $f(x)$ is the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Bagging Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaggingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_estimator, n_iter, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.models = # TODO \n",
    "        self.len_models = np.float(len(self.models))\n",
    "        \n",
    "        self.model_weights = # TODO\n",
    "    \n",
    "    def _get_random_choice(self, X, y):\n",
    "        # TODO \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        predictions = list()\n",
    "        # TODO \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = list()\n",
    "        # TODO \n",
    "        \n",
    "        return np.sum(\n",
    "            (predictions * self.model_weights).T,\n",
    "            axis=0\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.13810465036865\n",
      "    mae: 44.8311591276148,\n",
      "    r2: 0.5125504367055902\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 47.18119051778752\n",
      "    mae: 37.562764262853975,\n",
      "    r2: 0.5185530943562255\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.29030799180428\n",
      "    mae: 43.29458609843629,\n",
      "    r2: 0.5120940215262753\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 54.829700737551605\n",
      "    mae: 44.26763447175827,\n",
      "    r2: 0.5158041053722113\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.889377616718875\n",
      "    mae: 42.544781573685704,\n",
      "    r2: 0.531846949925868\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 56.67850709326859\n",
      "    mae: 46.78686224314351,\n",
      "    r2: 0.4312338566106785\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.85176290320033\n",
      "    mae: 42.398264938066035,\n",
      "    r2: 0.5171352056096121\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 56.9169086845541\n",
      "    mae: 47.08809675279553,\n",
      "    r2: 0.49515158664437897\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.07915123962693\n",
      "    mae: 42.95189114974093,\n",
      "    r2: 0.5150970124381291\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 55.54487019989398\n",
      "    mae: 45.319703511235325,\n",
      "    r2: 0.5183968031945675\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "do_cv(folds, SimpleBaggingModel(LinearRegression(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Using Sklearn\n",
    "\n",
    "Sklearn has a nice wrapper for simple bagging, both classifier and regressor. It resides at `sklearn.ensemble` module, `BaggingClassifier`, `BaggingRegressor`, `VotingClassifier` and `VotingRegressor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor, BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.09969883764457\n",
      "    mae: 44.745080551716086,\n",
      "    r2: 0.5132292549872977\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 47.58397508755066\n",
      "    mae: 38.26932479257507,\n",
      "    r2: 0.5102978076653342\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.328949569009154\n",
      "    mae: 43.312391848471314,\n",
      "    r2: 0.5113861895506542\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 54.992983075600854\n",
      "    mae: 44.03197556776964,\n",
      "    r2: 0.5129159495878475\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.884393362642584\n",
      "    mae: 42.612882750677166,\n",
      "    r2: 0.5319351825315326\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 57.374383153983906\n",
      "    mae: 47.15016072904597,\n",
      "    r2: 0.4171819542566956\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.88001103085017\n",
      "    mae: 42.505015017424434,\n",
      "    r2: 0.5166189060294248\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 57.26542372051097\n",
      "    mae: 47.37791559140956,\n",
      "    r2: 0.488950057104269\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.02884823033208\n",
      "    mae: 42.95027773473119,\n",
      "    r2: 0.5160156601842849\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 56.60443911276028\n",
      "    mae: 46.064596632885944,\n",
      "    r2: 0.4998475145324739\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: do_cv with BaggingRegressor, base_estimator = LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 57.80844324838222\n",
      "    mae: 48.73861299575631,\n",
      "    r2: 0.4641927726453505\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 48.217675647771706\n",
      "    mae: 38.881482535360384,\n",
      "    r2: 0.49716771797273895\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.86460921921472\n",
      "    mae: 46.59052675159114,\n",
      "    r2: 0.46381679109484175\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 58.47910968473439\n",
      "    mae: 49.41577286710758,\n",
      "    r2: 0.4492039006058257\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.37289231457794\n",
      "    mae: 45.83066583291244,\n",
      "    r2: 0.4868487845360434\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 59.13910729957115\n",
      "    mae: 50.065281283485426,\n",
      "    r2: 0.3807778816119701\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.666422272270886\n",
      "    mae: 46.29981399897339,\n",
      "    r2: 0.46433508042988714\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 59.03975704782762\n",
      "    mae: 48.93645295855194,\n",
      "    r2: 0.45679030757347705\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.782288817790125\n",
      "    mae: 46.215248861557846,\n",
      "    r2: 0.4644505465652633\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 58.30108570534128\n",
      "    mae: 49.80107024851205,\n",
      "    r2: 0.46941528656942244\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: do_cv with VotingRegressor, models: LinearRegression, Lasso, Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "There are 2 types of popular boosting: AdaBoost and Gradient Boosting, where each boosting is based on decision tree.\n",
    "\n",
    "## Adaptive Boosting (AdaBoost)\n",
    "\n",
    "### Main Idea\n",
    "Main idea for Adaptive Boosting is \n",
    "> iteratively train $T$ models, for each model $t \\in T$: train the model with **weigthed samples**, where samples with **less error will receive less weight** in next iteration. Then combine the prediction of each model $t$ with **weighted estimators**.\n",
    "\n",
    "### Deeper Understanding: the Algorithm\n",
    "Algorithm for AdaBoost is given by $^{1, 2}$, and simple implementation could be also be found online $^{3}$. \n",
    "\n",
    "Given $(x_1, y_1) ... (x_m, y_m)$ where $x_i \\in \\mathcal{X}$, $y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "Initialize: $D_1(i) = 1/m$ for $i = 1, ..., m.$\n",
    "\n",
    "For $t = 1...T$\n",
    "\n",
    "- Train weak learner using distribution $D_t$\n",
    "- Get weak hypothesis $h_t: \\mathcal{X} -> \\{-1, +1\\}$\n",
    "- Choose $\\alpha = \\frac{1}{2} \\text{ ln } \\frac{(1 - \\epsilon_t)}{\\epsilon_t}$\n",
    "- Update, for $i \\text{ = 1, ... , m}$:\n",
    "\n",
    "$$\n",
    "D_{t+1}(i) = \\frac{D_t(i)\\text{ exp }(-\\alpha_t y_i h_t (x_i))} {Z_t}\n",
    "$$\n",
    "\n",
    "Where $Z_t$ is normalization factor. Final output:\n",
    "\n",
    "$$\n",
    "H(x) = \\text{sign}(\\sum_{t=1}^{T} \\alpha_t h_t(x))\n",
    "$$\n",
    "\n",
    "However this algorithm is for training a classifier, for regressor refer to [4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=LinearRegression(copy_X=True,\n",
       "                                                  fit_intercept=True,\n",
       "                                                  n_jobs=None,\n",
       "                                                  normalize=False),\n",
       "                  learning_rate=1.0, loss='linear', n_estimators=10,\n",
       "                  random_state=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO define AdaBoostRegressor, n_estimators=10\n",
    "mdl = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.58461352363204\n",
      "    mae: 45.70915671410108,\n",
      "    r2: 0.5046237288597635\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 46.32977941873833\n",
      "    mae: 36.78358234262941,\n",
      "    r2: 0.5357722748522942\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.28447986824357\n",
      "    mae: 43.62177432705803,\n",
      "    r2: 0.5122007358967141\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 55.210424637037285\n",
      "    mae: 44.715309105399385,\n",
      "    r2: 0.509056486124728\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.24303854517125\n",
      "    mae: 43.148646918167266,\n",
      "    r2: 0.5255651213851553\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 57.719051722991\n",
      "    mae: 47.67595685821596,\n",
      "    r2: 0.4101585257803285\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 52.92267274869893\n",
      "    mae: 42.845432589246705,\n",
      "    r2: 0.5158386419584421\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 57.47681308397919\n",
      "    mae: 47.92605989400731,\n",
      "    r2: 0.48517011776834285\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.1490045545595\n",
      "    mae: 43.210170502869666,\n",
      "    r2: 0.513819886918703\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 55.99779556215826\n",
      "    mae: 45.779219265694174,\n",
      "    r2: 0.5105105791972346\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: do_cv with AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GBM)\n",
    "\n",
    "Gradient boosting works slightly different from Adaptive Boosting. Main differences between those two algorithm are: **how to fit each iteration model** and **how to determine estimator weights for each iteration**. Idea comes from Friedman$^{9}$, and now have been improved into *extreme* gradient boosting, for example: LightGBM$^{7}$, XGBoost$^{8}$, and CatBoost$^{10}$\n",
    "\n",
    "### Main Idea\n",
    "\n",
    "Gradient boosting main idea is to train next iteration model on residual of previous model as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO define GradientBoostingRegressor, n_estimators=10\n",
    "mdl = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 55.47491107519263\n",
      "    mae: 47.931418569695666,\n",
      "    r2: 0.5065771601937898\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 52.56444394990826\n",
      "    mae: 43.73903490279989,\n",
      "    r2: 0.40242178236903403\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.35330434322521\n",
      "    mae: 45.85119284616699,\n",
      "    r2: 0.5109397979959944\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 63.39114366471927\n",
      "    mae: 53.26895554133836,\n",
      "    r2: 0.3527880519487061\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.289577662852885\n",
      "    mae: 45.35429693697925,\n",
      "    r2: 0.5247353629381997\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 62.3771317980767\n",
      "    mae: 53.06529940099563,\n",
      "    r2: 0.3111134045227266\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.39010855604922\n",
      "    mae: 45.41101696828909,\n",
      "    r2: 0.5072482289524957\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 62.81946994086882\n",
      "    mae: 53.1134645599001,\n",
      "    r2: 0.38501160269023027\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 53.447388908885145\n",
      "    mae: 45.10348931123881,\n",
      "    r2: 0.5083456261791047\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 62.606078271113404\n",
      "    mae: 54.62348733751703,\n",
      "    r2: 0.38816481527521485\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: do_cv on GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Stacking is another *Meta - Feature* learning algorithm that \"stack\" models together. \n",
    "\n",
    "## Main Idea\n",
    "Stacking works by constructing network of models. We learn to *weight* each prediction result to construct final, more accurate prediction.\n",
    "\n",
    "![stacking](https://cdn-images-1.medium.com/max/1600/0*GHYCJIjkkrP5ZgPh.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackModel(object):\n",
    "    def __init__(self, models):\n",
    "        # TODO: define models and meta_learner as linear regression\n",
    "        self.models = \n",
    "        self.meta_learner = \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        predictions = list()\n",
    "        # TODO: define \n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = list()\n",
    "        # TODO: define and return final_predictions\n",
    "        \n",
    "        return final_prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    # make pipeline for each model, containing minmaxscaler as preprocessor and the model\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.StackModel at 0x7f555007c048>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 1\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 27.526870386874396\n",
      "    mae: 20.95588714658343,\n",
      "    r2: 0.8785102341960441\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 62.90063010602655\n",
      "    mae: 49.449384572691606,\n",
      "    r2: 0.14430183360388305\n",
      "    \n",
      "Cross Validation 2\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 25.420983563466706\n",
      "    mae: 19.170406795039924,\n",
      "    r2: 0.88897408044089\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 58.37330465132906\n",
      "    mae: 46.33887812408529,\n",
      "    r2: 0.4511951853681898\n",
      "    \n",
      "Cross Validation 3\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 26.087357228349216\n",
      "    mae: 20.07656384004358,\n",
      "    r2: 0.8861033068968689\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 68.81116456051153\n",
      "    mae: 52.91720167545122,\n",
      "    r2: 0.1616705027175802\n",
      "    \n",
      "Cross Validation 4\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 26.71528532801037\n",
      "    mae: 19.571672707095523,\n",
      "    r2: 0.8766252686460366\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 66.74097407750051\n",
      "    mae: 56.96343240600481,\n",
      "    r2: 0.30583379142640654\n",
      "    \n",
      "Cross Validation 5\n",
      "\n",
      "    ....train metrics....\n",
      "    rmse: 25.88798667067702\n",
      "    mae: 19.195143467230263,\n",
      "    r2: 0.884653751767361\n",
      "    \n",
      "\n",
      "    ....test metrics....\n",
      "    rmse: 65.07678596467794\n",
      "    mae: 51.924530867062415,\n",
      "    r2: 0.3389205760329138\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# TODO: do_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Schapire, Robert E. [*Explaining AdaBoost*](http://rob.schapire.net/papers/explaining-adaboost.pdf). Princeton University\n",
    "\n",
    "[2] Freund, Yoav & Schapire, Robert E. [*A Short Introduction to Boosting*](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf). AT&T Labs\n",
    "\n",
    "[3] Sicotte, Xavier Bourret. [*Adaboost: Implementation and Intuition*](https://xavierbourretsicotte.github.io/AdaBoost.html)\n",
    "\n",
    "[4] Drucker, Harris. [*Improving Regressors using Boosting Techniques*](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&rep=rep1&type=pdf)\n",
    "\n",
    "[5] Wikipedia. [Bootstraping (Statistics)](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))\n",
    "\n",
    "[6] Breiman, Leo. [*Bagging Predictors*](https://www.stat.berkeley.edu/~breiman/bagging.pdf). 1994. Technical Report No.421\n",
    "\n",
    "[7] Ke, Guolin et al. [*LightGBM: A Highly Efficient Gradient Boosting Decision Tree*](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf). 2017. NIPS\n",
    "\n",
    "[8] Chen, Tianqi & Guestrin, Carlos. [*XGBoost: A Scalable Tree Boosting System*](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf). 2016. KDD\n",
    "\n",
    "[9] Friedman, Jerome H. [*Greedy Function Approximation: A Gradient Boosting Machine*](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf). 1999. IMS 1999 Reitz Lecture.\n",
    "\n",
    "[10] Prokhorenkova, Liudmila et al. [*CatBoost: unbiased boosting with categorical features*](https://arxiv.org/pdf/1706.09516.pdf). 2019. ArXiv.\n",
    "\n",
    "[11] Wolpert, David H. [*Stacked Generalization*](https://www.sciencedirect.com/science/article/pii/S0893608005800231). 1992. Neural Networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
